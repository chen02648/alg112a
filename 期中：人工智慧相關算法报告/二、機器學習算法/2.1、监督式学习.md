# 监督式学习
  * **监督式学习**是机器学习的一种主要范式，其基本思想是通过训练模型来学习输入数据与对应的标签之间的映射关系。在监督式学习中，算法接收有标签的训练数据，通过学习这些数据的模式，使得模型能够对新的、未标记的数据做出预测。
  * 监督式学习包括两个主要部分：
    1. **训练阶段：** 模型接收有标签的训练数据集，学习输入特征和对应标签之间的映射关系。训练的目标是使模型能够准确地预测未见过的数据。
    2. **测试阶段：** 使用模型对新的、未标记的数据进行预测。通过比较模型的预测值与实际标签，评估模型在未见过的数据上的性能。
  * 监督式学习涵盖了许多不同的任务，其中包括：
    - **分类（Classification）：** 预测输入数据属于哪个类别。例如，判断一封电子邮件是垃圾邮件还是非垃圾邮件。
    - **回归（Regression）：** 预测输入数据的连续数值输出。例如，预测房屋的售价。
    - **目标检测（Object Detection）：** 在图像或视频中标识和定位多个物体。
    - **语义分割（Semantic Segmentation）：** 将图像划分为不同的语义区域，为每个像素分配相应的标签。
    - **自然语言处理（Natural Language Processing，NLP）：** 包括文本分类、命名实体识别、情感分析等任务。
***


# 線性回歸
  * 在統計學中，線性回歸（linear regression）是利用稱為線性回歸方程式的最小平方函數對一個或多個自變數和應變數之間關係進行建模的一種回歸分析。這種函數是一個或多個稱為回歸係數的模型參數的線性組合。只有一個自變數的情況稱為簡單回歸，大於一個自變數情況的叫做多元回歸（multivariable linear regression）
  * 在線性回歸中，數據使用線性預測函數來建模，並且未知的模型參數也是通過數據來估計。這些模型被叫做線性模型。[2]最常用的線性回歸建模是給定X值的y的條件均值是X的仿射函數。不太一般的情況，線性回歸模型可以是一個中位數或一些其他的給定X的條件下y的條件分布的分位數作為X的線性函數表示。像所有形式的回歸分析一樣，線性回歸也把焦點放在給定X值的y的條件機率分布，而不是X和y的聯合機率分布（多元分析領域）。
  * ### 簡單線性回歸
    * 簡單線性迴歸探討兩個連續變數之間的關係，這兩個變數分別被命名為 independent variable（自變數）及 dependent variable （應變數），各自以 x 和 y 來表示。
      * 自變數 x，有時又稱做 predictor variable 或是 explanatory variable。
      * 應變數 y，有時又稱做 response variable 或是 outcome variable。其值會隨著 x 的改變而變化。
    * 而這兩變數的關係，又可以分為下列兩種；Deterministic / Functional relationships （定性關係／函數關係）及 Statistical relationships（統計關係）。
    * 簡單線性回歸的數學模型可以表示為：y = b0 + b1x，其中y是因變數，x是自變數，b0和b1是回歸係數。簡單線性回歸分析的目的是找到最佳的回歸係數，以最小化實際觀測值和預測值之間的差異。
  * ### 多元線性回歸
    * 多元線性回歸是一種統計分析方法，用於探討多個自變數和一個依變數之間的關係。它可以用來預測依變數的值，並且可以確定每個自變數對依變數的影響。多元線性回歸的模型假設自變數和依變數之間存在線性關係，並且自變數之間不存在多重共線性。多元線性回歸的結果可以用於預測新的依變數值，並且可以用於確定哪些自變數對依變數的影響最大。在進行多元回歸分析時，我們必須先確保資料符合以下特性：
      * **自變項與依變項成線性關係**
        與簡單線性回歸相同，自變項和依變項之間的關係是按比例的。也就是說今天能夠以一條直線，直接代表出自變項與依變項之間關聯。
      * **變異同質性**
        意旨在自變項中任意取值時，其每個殘差的方差應該是平穩的，而非有巨大的落差、變異。
      * **誤差獨立性**
        所有殘差之間互不關聯。
      * **誤差常態性**
        殘差的分布，應屬於正態常態分布。
      * **無多重貢線性**
        自變項之間不應有相關性。

***

# 支持向量機（Support Vector Machines）
   * 支持向量機（Support Vector Machine，SVM）是一種強大的機器學習模型，通常用於分類和回歸問題。SVM 的主要目標是找到一個能夠區分不同類別的超平面，同時最大程度地將兩個類別之間的邊界擴大。這個超平面被設計為使最靠近它的數據點（稱為支持向量）之間的距離最大化。
   * 原始SVM演算法是由蘇聯數學家弗拉基米爾·瓦普尼克和亞歷克塞·澤范蘭傑斯於1963年發明的。1992年，伯恩哈德·E·博瑟（Bernhard E. Boser）、伊莎貝爾·M·蓋昂（Isabelle M. Guyon）和瓦普尼克提出了一種通過將核技巧應用於最大間隔超平面來建立非線性分類器的方法。當前標準的前身（軟間隔）由科琳娜·科特斯和瓦普尼克於1993年提出，並於1995年發表。
   * ### 基本概念
     * **超平面**： 在二維空間中，超平面是一條直線，而在更高維度的空間中，它是一個平面或者超平面。SVM 通常用於高維數據。
     * **支持向量**： 這些是最接近超平面的數據點。它們在定義超平面並計算距離時起到關鍵作用。
     * **分類問題**： 在分類問題中，SVM 的目標是找到一個超平面，將不同類別的數據點區分開來。這個超平面使得兩個類別之間的邊界最大化。
     * **回歸問題**： 除了用於分類，SVM 還可以應用於回歸問題。在回歸中，SVM 的目標是找到一個超平面，使預測值和實際值之間的誤差最小化。
     * **核函數**： 當數據不是線性可分的時候，SVM 使用核函數將數據映射到更高維的空間，使其在該空間中變得線性可分。
     * **軟間隔**： 在實際問題中，數據通常不是完全線性可分的。SVM 引入了軟間隔，允許一些數據點被分錯，以更好地適應實際數據。
  * ### 动机
    * 將資料進行分類是機器學習中的一項常見任務。 假設某些給定的資料點各自屬於兩個類之一，而目標是確定新資料點將在哪個類中。對於支持向量機來說，資料點被視為p維向量，而我們想知道是否可以用（p-1）維超平面來分開這些點。這就是所謂的線性分類器。可能有許多超平面可以把資料分類。最佳超平面的一個合理選擇是以最大間隔把兩個類分開的超平面。因此，我們要選擇能夠讓到每邊最近的資料點的距離最大化的超平面。如果存在這樣的超平面，則稱為最大間隔超平面，而其定義的線性分類器被稱為最大間隔分類器，或者叫做最佳穩定性感知器
  * ### 应用
    * 用於文字和超文字的分類，在歸納和直推方法中都可以顯著減少所需要的有類標的樣本數。
    * 用於圖像分類。實驗結果顯示：在經過三到四輪相關回饋之後，比起傳統的查詢最佳化方案，支持向量機能夠取得明顯更高的搜尋準確度。這同樣也適用於圖像分割系統，比如使用Vapnik所建議的使用特權方法的修改版本SVM的那些圖像分割系統。
    * 用於手寫字型辨識。
    * 用於醫學中分類蛋白質，超過90%的化合物能夠被正確分類。基於支持向量機權重的置換測試已被建議作為一種機制，用於解釋的支持向量機模型。 支持向量機權重也被用來解釋過去的SVM模型。為辨識模型用於進行預測的特徵而對支持向量機模型做出事後解釋是在生物科學中具有特殊意義的相對較新的研究領域。

***

# 決策樹和隨機森林
  * 決策樹（Decision Tree） 和 隨機森林（Random Forest） 都是用於機器學習中的強大模型，主要用於分類和回歸問題。
  * ### 决策树
    * 機器學習中，決策樹是一個預測模型；他代表的是對象屬性與對象值之間的一種映射關係。樹中每個節點表示某個對象，而每個分叉路徑則代表某個可能的屬性值，而每個葉節點則對應從根節點到該葉節點所經歷的路徑所表示的對象的值。決策樹僅有單一輸出，若欲有複數輸出，可以建立獨立的決策樹以處理不同輸出。 數據挖掘中決策樹是一種經常要用到的技術，可以用於分析數據，同樣也可以用來作預測。從數據產生決策樹的機器學習技術叫做決策樹學習,通俗說就是決策樹。
    * 一個決策樹包含三種類型的節點：
      * 決策節點：通常用矩形框來表示
      * 機會節點：通常用圓圈來表示
      * 終結點：通常用三角形來表示
    * 決策樹學習也是數據挖掘中一個普通的方法。在這裡，每個決策樹都表述了一種樹型結構，它由它的分支來對該類型的對象依靠屬性進行分類。每個決策樹可以依靠對源資料庫的分割進行數據測試。這個過程可以遞歸式的對樹進行修剪。 當不能再進行分割或一個單獨的類可以被應用於某一分支時，遞歸過程就完成了。另外，隨機森林分類器將許多決策樹結合起來以提升分類的正確率。
    * 決策樹同時也可以依靠計算條件概率來構造。決策樹如果依靠數學的計算方法可以取得更加理想的效果。
    * **决策树有几种产生方法**：
      * 分類樹分析是當預計結果可能為離散類型（例如三個種類的花，輸贏等）使用的概念。
      * 回歸樹分析是當局域結果可能為實數（例如房價，患者住院時間等）使用的概念。
      * CART分析是結合了上述二者的一個概念。CART是Classification And Regression Trees的縮寫.
      * CHAID（Chi-Square Automatic Interaction Detector）
    * **建立方法**
      * 以資料母群體為根節點。
      * 作單因子變異數分析等，找出變異量最大的變項作為分割準則。（決策樹每個葉節點即為一連串法則的分類結果。）
      * 若判斷結果的正確率或涵蓋率未滿足條件，則再依最大變異量條件長出分岔。
    * #### 优点：
      1. **易解释和可视化：** 决策树的结构清晰，易于理解和解释。可以通过可视化呈现出树形结构，方便展示给非专业人员。
      2. **不需要特征缩放：** 决策树不需要对特征进行缩放，因此对于不同尺度的特征处理较为简单。
      3. **处理混合数据类型：** 决策树能够处理混合类型的数据，包括连续型和离散型特征。
      4. **适用于小规模数据集：** 在数据集较小且特征数量不多的情况下，决策树的构建和预测速度较快。
    * #### 缺点：
      1. **过拟合：** 决策树容易过拟合，特别是在处理复杂关系的数据时，可能生成过于复杂的树结构。
      2. **稳定性差：** 数据的小变动可能导致决策树发生较大变化，对数据的扰动敏感。
      3. **局部最优解：** 决策树采用贪心算法，容易陷入局部最优解，可能不能达到全局最优。

  * ### 随机森林
    * 随机森林是一种基于集成学习的机器学习算法，它建立在多个决策树的基础上，通过对它们的输出进行平均或投票来提高整体模型的性能和鲁棒性。
    * **主要特点**
      * **集成學習**： 隨機森林是一種集成學習算法，通過結合多個弱學習器（決策樹），以提高整體的準確性和泛化能力。
      * **隨機性**： 在建構每棵樹的過程中引入了隨機性。隨機森林通常使用隨機抽樣的方式從訓練集中選取子樣本，這樣每棵樹的訓練數據都是不同的。
      * **隨機特徵選擇**： 在每次節點分裂的時候，隨機森林從所有特徵中選擇一個子集進行分裂，這種方式被稱為隨機特徵選擇。這有助於減少模型的相關性，提高泛化能力。
      * **投票機制**： 對於分類問題，隨機森林通常使用投票機制，每棵樹投票給一個類別，最終選擇獲得最多票的類別。在回歸問題中，則取多棵樹預測值的平均值。
      * **高鲁棒性**： 隨機森林對噪聲和過度擬合的抵抗力很強，這是由於每棵樹使用不同的訓練子集和特徵子集。
      * **易於調參**： 隨機森林相對於單一決策樹，對超參數的選擇不太敏感，使得模型的調參過程相對容易。
    * **工作原理**
      * **隨機樣本**： 從訓練數據中隨機選擇一定數量的樣本構建每棵樹。
      * **隨機特徵**： 在每次分裂節點的時候，隨機選擇一部分特徵進行分裂。
      * **構建樹**： 通過不斷地選擇特徵和分裂節點，構建多棵樹。
      * **投票**： 對於分類問題，每棵樹進行預測，採用投票機制選擇最終的類別。對於回歸問題，取多棵樹預測值的平均值。
      * **提高泛化能力**： 由於每棵樹都是在不同的數據子集上訓練的，因此隨機森林具有更好的泛化能力。
    * #### 优点：
      1. **高准确性：** 随机森林通常能够取得较高的准确性，特别在处理大规模数据集和高维数据时表现良好。
      2. **降低过拟合风险：** 通过随机抽样和随机特征选择，随机森林降低了单颗决策树过拟合的风险，提高了模型的泛化能力。
      3. **处理大规模数据：** 随机森林能够有效地处理大规模数据集，分布式训练时也比较容易实现。
      4. **不需特征缩放：** 与决策树类似，随机森林不需要对特征进行缩放。
    * #### 缺点：
      1. **模型复杂性：** 随机森林生成的模型较为复杂，不如决策树容易解释。
      2. **计算开销：** 由于需要构建多颗决策树，训练和预测的计算开销相对较大。
      3. **可能不适用于高度稀疏的数据：** 在高度稀疏的数据集上，随机森林的表现可能不如其他模型。

***

# 深度學習（簡介）
  * **深度学习**是一种机器学习的分支，其核心思想是通过模拟人脑神经网络的结构和工作方式，使用深层次的神经网络来解决复杂的模式识别和决策问题。深度学习的主要特点是模型的层数较多，通常包含多个隐层。
  * ### 主要特點：
    1. **神经网络结构：** 深度学习中的模型通常由多个层次组成，包括输入层、隐层（可以有多层），以及输出层。每一层都由多个神经元（节点）组成。
    2. **反向传播：** 深度学习使用反向传播算法来训练模型。这是一种基于梯度下降的优化算法，通过不断调整模型参数，使得模型的预测结果逼近实际值。
    3. **特征学习：** 深度学习具有自动学习特征的能力。在训练过程中，模型能够从原始数据中学习到对任务有用的特征，而无需手动提取特征。
    4. **大规模数据：** 深度学习对大规模数据的需求较大，通常需要足够数量的标注数据用于模型的训练。
    5. **端到端学习：** 深度学习支持端到端的学习方式，直接从原始输入到最终输出进行训练，无需手动设计复杂的特征提取器。
  * ### 常见应用：
    1. **图像识别：** 深度学习在图像识别任务中取得了显著的成功，例如卷积神经网络（CNN）用于图像分类、目标检测等。
    2. **语音识别：** 语音识别是深度学习的另一个成功应用领域，递归神经网络（RNN）和长短时记忆网络（LSTM）被广泛用于语音识别系统。
    3. **自然语言处理：** 深度学习在自然语言处理任务中取得了显著的成果，如机器翻译、情感分析、文本生成等。
    4. **游戏与强化学习：** 深度学习在强化学习领域取得了很大进展，如AlphaGo通过深度强化学习战胜人类围棋冠军。
    5. **医学影像分析：** 在医学领域，深度学习被用于图像分析、病理诊断等。
    <br>深度学习的发展得益于大规模数据的可用性、强大的计算能力以及新的神经网络结构的提出。尽管深度学习在许多领域取得了成功，但其模型复杂性和对大量数据的需求也带来了一些挑战。

