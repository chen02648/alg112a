# 非监督式学习
  * **非监督式学习**是机器学习的一种范式，与监督式学习不同，非监督式学习的训练数据没有标签。在这种情况下，算法被要求自行发现数据中的结构、模式或规律，而不是通过已知的标签进行学习。
  * 非监督式学习通常包括以下主要任务：
    1. **聚类（Clustering）：** 将数据集中的样本划分为若干组，每组内的样本相似度较高，而不同组之间的相似度较低。
    2. **降维（Dimensionality Reduction）：** 减少输入数据的特征维度，保留数据的主要信息。常见的降维方法有主成分分析（PCA）和 t-分布邻域嵌入（t-SNE）等。
    3. **关联规则学习（Association Rule Learning）：** 发现数据中的关联规则，即数据中不同属性之间的关系。
    4. **生成模型（Generative Modeling）：** 建立数据的生成模型，使得可以从模型中生成与训练数据相似的新数据。
    5. **异常检测（Anomaly Detection）：** 识别数据中的异常或离群值。
  * 非监督式学习的应用场景广泛，尤其在数据探索、无监督特征学习和模式识别等领域中得到了应用。它有助于揭示数据的内在结构，发现隐藏在数据背后的潜在模式，为进一步的数据分析提供基础。
***

# K均值聚類
  * **K均值聚类（K-Means Clustering）** 是一种常见的聚类算法，用于将数据集中的样本划分为K个不同的组或簇，使得每个样本属于与其最近的簇中。这种算法以其简单、易于理解的特点而广泛应用。
  * 使得k-平均演算法效率很高的兩個關鍵特徵同時也被經常被視為它最大的缺陷：
    * 聚類數目k是一個輸入參數。選擇不恰當的k值可能會導致糟糕的聚類結果。這也是為什麼要進行特徵檢查來決定資料集的聚類數目了。收斂到局部最佳解，可能導致「反直觀」的錯誤結果。
    * k-平均演算法的一個重要的局限性即在於它的聚類模型。這一模型的基本思想在於：得到相互分離的球狀聚類，在這些聚類中，均值點趨向收斂於聚類中心。 一般會希望得到的聚類大小大致相當，這樣把每個觀測都分配到離它最近的聚類中心（即均值點）就是比較正確的分配方案。
    * k-平均聚類的結果也能理解為由均值點生成的Voronoi cells。
  * ### K均值聚类的步骤：
    1. **初始化：** 随机选择K个初始质心，每个质心代表一个聚类中心。
    2. **分配：** 将每个样本分配到与其最近的质心所代表的簇中，形成K个簇。
    3. **更新质心：** 对于每个簇，计算其所有样本的平均值，将该平均值作为新的质心。
    4. **迭代：** 重复执行步骤2和步骤3，直到质心不再变化或变化小于一个阈值，或者达到预定的最大迭代次数。
  * ### K均值聚类的特点：
    - **硬聚类：** 每个样本被分配到一个簇，形成非重叠的簇。
    - **对异常值敏感：** K均值对异常值敏感，可能会导致质心偏离簇的实际中心。
    - **需要指定簇数K：** 在使用K均值聚类之前，通常需要事先指定簇的数量K。
    - **速度较快：** K均值聚类是一种迭代算法，通常具有较快的收敛速度。
  * ### 示例代码（使用Python的Scikit-Learn库）：

```python
from sklearn.cluster import KMeans
import numpy as np

# 创建样本数据
X = np.array([[1, 2], [5, 8], [1.5, 1.8], [8, 8], [1, 0.6], [9, 11]])

# 创建K均值聚类模型
kmeans = KMeans(n_clusters=2)

# 训练模型
kmeans.fit(X)

# 获取簇中心和预测标签
centroids = kmeans.cluster_centers_
labels = kmeans.labels_

print("簇中心：", centroids)
print("预测标签：", labels)
```

在上述代码中，我们使用K均值聚类将样本数据分为2个簇。结果包括每个簇的中心（centroids）和每个样本的预测标签（labels）。
  * ### 目前应用
    K均值聚类算法在许多领域中都得到了广泛的应用，其中一些主要方面包括：
      1. **图像分割：** K均值聚类可用于图像分割，将图像分成具有相似特征的区域。这在计算机视觉和图像处理中是一个常见的应用。
      2. **市场细分：** 在市场营销中，K均值聚类可以用于将消费者细分为不同的群体，从而更好地了解目标市场。
      3. **推荐系统：** 在推荐系统中，K均值聚类可以帮助将用户分成不同的群体，从而更好地个性化推荐产品或服务。
      4. **无监督特征学习：** K均值聚类可以用于无监督学习中，帮助发现数据中的潜在结构，并生成新的特征。
      5. **网络分析：** 在社交网络或其他网络数据中，K均值聚类可以用于发现相似性较高的节点群体。
      6. **生物信息学：** 在生物学领域，K均值聚类被用于基因表达数据的聚类分析，有助于发现潜在的基因模式。
      7. **语音和音频分析：** K均值聚类可以用于音频信号的聚类，例如将相似的音频片段分为不同的簇。
      8. **金融领域：** 在金融领域，K均值聚类可以用于客户分群、风险管理和欺诈检测。

# 主成分分析（Principal Component Analysis）
  * **主成分分析（Principal Component Analysis，PCA）** 是一种常用的降维技术，用于发现数据中的主要结构，并将数据投影到新的坐标轴上，以减少数据的维度。PCA通过找到数据中的主成分（主要方向）来实现降维，从而更好地表示原始数据的变化。
    
    * 它借助于一个正交变换，将其分量相关的原随机向量转化成其分量不相关的新随机向量，这在代数上表现为将原随机向量的协方差阵变换成对角形阵，在几何上表现为将原坐标系变换成新的正交坐标系，使之指向样本点散布最开的p 个正交方向，然后对多维变量系统进行降维处理，使之能以一个较高的精度转换成低维变量系统，再通过构造适当的价值函数，进一步把低维系统转化成一维系统。
      
    * 主成分分析的原理是设法将原来变量重新组合成一组新的相互无关的几个综合变量，同时根据实际需要从中可以取出几个较少的总和变量尽可能多地反映原来变量的信息的统计方法叫做主成分分析或称主分量分析，也是数学上处理降维的一种方法。主成分分析是设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的方差来表达，即Va（rF1）越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现再F2中，用数学语言表达就是要求Cov（F1,F2）=0，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。
  * ### 主成分分析的步骤：
    1. **数据标准化：** 如果数据的不同特征具有不同的尺度，首先需要对数据进行标准化，使得每个特征的方差相等。
    2. **计算协方差矩阵：** 对标准化后的数据计算协方差矩阵，该矩阵描述了数据中不同特征之间的关系。
    3. **计算特征值和特征向量：** 对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征向量表示数据中的主成分方向，而特征值表示该方向上的方差大小。
    4. **选择主成分：** 选择具有最大特征值的前k个特征向量作为主成分，其中k是希望降维后的维度。
    5. **投影：** 将原始数据投影到所选的主成分上，得到降维后的数据。
  * ### 主成分分析的应用：
    1. **降维：** PCA广泛用于降低高维数据的维度，减少数据的复杂性，同时保留数据的主要特征。
    2. **特征提取：** PCA可以用于提取数据中最重要的特征，有助于理解数据的结构。
    3. **去噪：** PCA通过保留主要方差来去除数据中的噪声，提高数据的质量。
    4. **可视化：** PCA可以将高维数据投影到低维空间，从而实现数据的可视化。
    5. **模型性能改善：** 在一些情况下，通过使用PCA降维，可以改善机器学习模型的性能，尤其是在高维数据集上。
  * ### 示例代码（使用Python的Scikit-Learn库）：

```python
from sklearn.decomposition import PCA
import numpy as np

# 创建样本数据
X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# 创建PCA模型
pca = PCA(n_components=2)

# 训练模型并进行数据转换
X_pca = pca.fit_transform(X)

print("原始数据：\n", X)
print("降维后的数据：\n", X_pca)
```

  * 在上述代码中，我们使用PCA将3维数据降维为2维。结果包括降维后的数据`X_pca`。


# 自編碼器
  * **自编码器（Autoencoder）** 是一种神经网络结构，用于学习输入数据的紧凑表示（编码），同时能够重建原始输入数据。自编码器由一个编码器和一个解码器组成，其中编码器负责将输入数据映射到紧凑的表示，而解码器则负责从这个表示中重建原始输入。
  * ### 自编码器的结构：
    1. **编码器（Encoder）：** 将输入数据映射到紧凑的表示。通常是由全连接层（Dense Layer）组成的神经网络。
    2. **解码器（Decoder）：** 从紧凑表示中重建原始输入数据。也是由全连接层组成的神经网络，结构与编码器相反。
    3. **损失函数（Loss Function）：** 衡量重建数据与原始输入数据之间的差异，通常使用均方差（MSE）。
  * ### 自编码器的训练过程：
    1. 将输入数据通过编码器映射为紧凑表示。
    2. 将紧凑表示通过解码器重建为原始输入数据。
    3. 计算重建数据与原始输入数据的损失。
    4. 使用梯度下降等优化算法，调整网络参数以减小损失。
    5. 重复以上步骤直至收敛。
  * ### 自编码器的应用：
    1. **降维：** 自编码器可用于无监督降维，学习数据的紧凑表示，去除冗余信息。
    2. **去噪：** 自编码器通过学习在有噪声的输入上重建原始数据，具有去噪的效果。
    3. **生成模型：** 变分自编码器（Variational Autoencoder）等改进型自编码器可用于生成模型，生成与训练数据相似的新样本。
    4. **特征学习：** 自编码器可用于学习输入数据的有用特征，提取数据中的潜在结构。
  * ### 示例代码（使用Python的Keras库）：

```python
from keras.layers import Input, Dense
from keras.models import Model

# 定义编码器
input_layer = Input(shape=(784,))
encoded = Dense(128, activation='relu')(input_layer)

# 定义解码器
decoded = Dense(784, activation='sigmoid')(encoded)

# 构建自编码器模型
autoencoder = Model(input_layer, decoded)

# 编译模型
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# 训练模型，假设X_train是训练数据
autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True)
```
  * 上述代码中，我们使用Keras建立了一个简单的自编码器，输入数据的维度是784。模型通过编码器将数据映射到128维的表示，然后通过解码器重建原始输入数据。模型的损失函数使用均方差。在训练过程中，模型学习将输入数据通过编码器编码为紧凑表示，然后通过解码器重建为原始数据。
